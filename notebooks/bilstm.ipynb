{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tfn': conda)",
   "metadata": {
    "interpreter": {
     "hash": "cb0a7f5e682796f5c967770f1634aadd0c7e9a7fac4e07aebb6c721f3c61cba0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.2.0-dev20200508\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "# from attention import Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import data_helper\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1    17244\n0    17210\nName: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_path='../data/train_preprocessed.csv'\n",
    "\n",
    "data_df=pd.read_csv(data_path)[['text','cutted_text','label']]\n",
    "\n",
    "train_df, test_df=train_test_split(data_df,test_size=0.1,random_state=1)\n",
    "print(train_df['label'].value_counts()) # 分布均匀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "** Load tokenzier from:  ../data/tokenizer.pickle\n",
      "** Total different words: 73745.\n",
      "* X shape  (34454, 128)\n",
      "* Y shape  (34454, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path='../data/tokenizer.pickle'\n",
    "tokenizer_mode='load'\n",
    "max_num_words=100000\n",
    "max_seq_len=128\n",
    "\n",
    "X=train_df['cutted_text'].values\n",
    "# Y= train_df['label'].values # np.reshape(train_df['label'].values,(-1,1))\n",
    "Y = pd.get_dummies(train_df['label']).values\n",
    "\n",
    "if not os.path.exists(tokenizer_path):\n",
    "    tokenizer_mode = 'create'\n",
    "\n",
    "X,lang_tokenizer=data_helper.tokenize(X,mode=tokenizer_mode,path=tokenizer_path,max_num_words=max_num_words,max_sequence_len=max_seq_len)\n",
    "\n",
    "print('* X shape ', X.shape)\n",
    "print('* Y shape ', Y.shape)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 128, 128)          12800000  \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128, 256)          263168    \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 256)               394240    \n_________________________________________________________________\ndense (Dense)                (None, 64)                16448     \n_________________________________________________________________\ndropout (Dropout)            (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 130       \n=================================================================\nTotal params: 13,473,986\nTrainable params: 13,473,986\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 128\n",
    "dropout = 0.2\n",
    "lstm_units = 128\n",
    "regularizer_factor = 0.005\n",
    "output_units=2\n",
    "\n",
    "# build model\n",
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "                max_num_words, embedding_dims, input_length=max_seq_len),\n",
    "            # tf.keras.layers.SpatialDropout1D(dropout),\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "                lstm_units, return_sequences=True, kernel_regularizer=regularizers.l2(regularizer_factor))),\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "                int(lstm_units), kernel_regularizer=regularizers.l2(regularizer_factor))),\n",
    "            tf.keras.layers.Dense(\n",
    "                int(lstm_units/2), activation='relu', kernel_regularizer=regularizers.l2(regularizer_factor)),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(output_units, activation='softmax'),\n",
    "\n",
    "        ])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\n",
    "              tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "485/485 [==============================] - ETA: 0s - loss: 0.6128 - precision: 0.9422 - recall: 0.9422\n",
      "Epoch 00001: val_loss improved from inf to 0.14469, saving model to ../saved_models\\bilstm.h5\n",
      "485/485 [==============================] - 92s 189ms/step - loss: 0.6128 - precision: 0.9422 - recall: 0.9422 - val_loss: 0.1447 - val_precision: 0.9791 - val_recall: 0.9791\n",
      "Epoch 2/3\n",
      "485/485 [==============================] - ETA: 0s - loss: 0.1000 - precision: 0.9879 - recall: 0.9879\n",
      "Epoch 00002: val_loss did not improve from 0.14469\n",
      "485/485 [==============================] - 86s 176ms/step - loss: 0.1000 - precision: 0.9879 - recall: 0.9879 - val_loss: 0.1491 - val_precision: 0.9826 - val_recall: 0.9826\n",
      "Epoch 3/3\n",
      "485/485 [==============================] - ETA: 0s - loss: 0.1000 - precision: 0.9902 - recall: 0.9902\n",
      "Epoch 00003: val_loss improved from 0.14469 to 0.09124, saving model to ../saved_models\\bilstm.h5\n",
      "485/485 [==============================] - 92s 189ms/step - loss: 0.1000 - precision: 0.9902 - recall: 0.9902 - val_loss: 0.0912 - val_precision: 0.9837 - val_recall: 0.9837\n"
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "batch_size=64\n",
    "val_split=0.1 # 验证集划分\n",
    "checkpoint_path='../saved_models/bilstm.h5'\n",
    "\n",
    "save_model_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path, monitor='val_loss', mode='auto', save_best_only=True, save_weights_only=False, verbose=1, save_freq='epoch')\n",
    "callbacks=[save_model_cb]\n",
    "\n",
    "history=model.fit(x=X,y=Y,batch_size=batch_size,epochs=epochs,callbacks=callbacks,validation_split=val_split)\n",
    "\n",
    "# model.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "** Load tokenzier from:  ../data/tokenizer.pickle\n",
      "** Total different words: 73745.\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 0.0811 - precision: 0.9841 - recall: 0.9841\n",
      "train f1  0.9901960492134094\n",
      "val f1  0.983749270439148\n",
      "test_f1  0.9840689301490784\n"
     ]
    }
   ],
   "source": [
    "def f1_score(precision,recall):\n",
    "    return 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "train_f1=f1_score(history.history['precision'][-1],history.history['recall'][-1])\n",
    "val_f1=f1_score(history.history['val_precision'][-1],history.history['val_recall'][-1])\n",
    "\n",
    "\n",
    "test_X,_=data_helper.tokenize(test_df['cutted_text'].values,mode='load',path=tokenizer_path,max_num_words=max_num_words,max_sequence_len=max_seq_len)\n",
    "test_Y=pd.get_dummies(test_df['label']).values\n",
    "\n",
    "_,p,r=model.evaluate(test_X,test_Y)\n",
    "\n",
    "print('train f1 ',train_f1)\n",
    "print('val f1 ',val_f1)\n",
    "print('test_f1 ',f1_score(p,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}